{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shannon Entropy, Joint Entropy, Conditional Entropy, Mutual Information, Relative Entropy(KL Divergence)\n",
    "\n",
    "This page contains a simple example of the above mentioned four concepts. It would be helpful in the next tutorial where we would discuss Von Neumann Entropy, Holevo Bound and POVMs. \n",
    "\n",
    "Let us use the following table where the joint probability distribution of two random variables $X$ and $Y$ are given:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|      | Y=0  |Y=1 |Y=2     |\n",
    "|------|------|------|------|\n",
    "|X=0   |0   | 1/8 |  1/4    |\n",
    "|X=1   |1/16   | 0 |  1/16    |\n",
    "|X=2   |3/8   | 1/8 |  0    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the **Shannon entropy** of this system is given by the following equation: \n",
    ">$H(X, Y) = -\\sum_{x \\in X, y \\in Y}{p(x, y)\\log_2 p(x, y)}$\n",
    "\n",
    "The value in this case is: \n",
    ">$H(X, Y) = -0\\log_2 0 -\\frac{1}{8}\\log_2\\frac{1}{8}-\\frac{1}{4}\\log_2\\frac{1}{4}-\\frac{1}{16}\\log_2\\frac{1}{16}-0\\log_2 0-\\frac{1}{16}\\log_2\\frac{1}{16}-\\frac{3}{8}\\log_2\\frac{3}{8}-\\frac{1}{8}\\log_2\\frac{1}{8}-0\\log_2 0 = 2.2806$\n",
    "\n",
    "Note: $0\\log 0 = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual entropies could be found by marginalizing one variable with all values of the other one. \n",
    "\n",
    ">$p(X=0) = \\sum_{y}p(X=0, Y=y) = 0 + \\frac{1}{8} + \\frac{1}{4} = \\frac{3}{8}$\n",
    "\n",
    "Similarly, $p(X=1) = \\frac{1}{8}, p(X=2)=\\frac{4}{8}, p(Y=0)=\\frac{7}{16}, p(Y=1)=\\frac{1}{4}, p(Y=2)=\\frac{5}{16}$\n",
    "Now, \n",
    ">$H(X) = -\\frac{3}{8}\\log_2\\frac{3}{8} -\\frac{1}{8}\\log_2\\frac{1}{8} -\\frac{4}{8}\\log_2\\frac{4}{8} = 1.405$\n",
    "\n",
    "And,\n",
    ">$H(Y) = -\\frac{7}{16}\\log_2\\frac{7}{16} -\\frac{1}{4}\\log_2\\frac{1}{4} -\\frac{5}{16}\\log_2\\frac{5}{16} = 1.546$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, **the conditional entropies** would be: \n",
    ">$H(X|Y) = H(X, Y) - H(Y) = 2.2806 - 1.546 = .7346$\n",
    "\n",
    "And,\n",
    ">$H(Y|X) = H(X, Y) - H(X) = 2.2806 - 1.405 = .8756$\n",
    "\n",
    "Now, the **mutual information** would be: \n",
    ">$I(X;Y) = H(X) - H(X|Y) = 1.405 - .7346 = .6704$\n",
    "\n",
    "This value would be same if was calculated with $H(Y)$ and $H(Y|X)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the **relative entropy**, we must find the individual probability distribution of the two variables by marginalization. \n",
    "$$\n",
    "P(X) = \\{\\frac{3}{8}, \\frac{1}{8}, \\frac{1}{2}\\} \\\\\n",
    "P(Y) = \\{\\frac{7}{16}, \\frac{1}{4}, \\frac{5}{16}\\} \\\\\n",
    "$$\n",
    "Then the relative entropy of these two distributions would be: \n",
    "$$\n",
    "D(P \\| Q) = \\sum_{x \\in X}P(x) \\log{\\frac{P(x)}{Q(x)}}\n",
    "$$  \n",
    "In our case, this would be: \n",
    "\n",
    "$$\n",
    "D(X \\| Y) = P(X=0) * \\log_2 \\frac{P(X=0)}{P(Y=0)} + P(X=1) * \\log_2 \\frac{P(X=1)}{P(Y=1)} + P(X=2)* \\log_2 \\frac{P(X=2)}{P(Y=2)}\n",
    "$$\n",
    "Calculating, we get:\n",
    "$$\n",
    "D(X \\| Y) = .375 * \\log_2 \\frac{.375}{.4375} + .125 * \\log_2 \\frac{.125}{.25} + .5 * \\log_2 \\frac{.5}{.3125} = .1310\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
